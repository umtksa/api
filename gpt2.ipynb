{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets"
      ],
      "metadata": {
        "id": "yszQXlBwI_cM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wikiden url ile tek sayfa çalmaca\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "# Data klasörünü oluştur\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data')\n",
        "\n",
        "while True:\n",
        "    # Kullanıcıdan Wikipedia URL'sini al\n",
        "    url = input(\"URL or exit : \")\n",
        "\n",
        "    # Çıkış komutunu kontrol et\n",
        "    if url.lower() in ['exit', 'quit']:\n",
        "        print(\"Program sonlandırılıyor.\")\n",
        "        break\n",
        "\n",
        "    # Sayfa içeriğini al\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(\"URL'den veri alınamadı.\")\n",
        "        continue\n",
        "\n",
        "    # Sayfa içeriğini BeautifulSoup ile ayrıştır\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Sayfa başlığını al\n",
        "    title = soup.find('h1', {'id': 'firstHeading'}).get_text()\n",
        "\n",
        "    # Ana içerik kısmını seç (Vikipedi sayfalarındaki ana metin genellikle <p> etiketlerinde bulunur)\n",
        "    content = soup.find_all('p')\n",
        "\n",
        "    # Metni birleştir\n",
        "    text = \"\\n\".join([para.get_text().strip() for para in content])\n",
        "\n",
        "    # Boş satırları temizle\n",
        "    cleaned_text = \"\\n\".join(line for line in text.splitlines() if line.strip())\n",
        "\n",
        "    # Başlık için geçerli bir dosya adı oluştur\n",
        "    filename = f\"{title}.txt\".replace('/', '_').replace('\\\\', '_')\n",
        "\n",
        "    # Dosyayı data klasörüne kaydet\n",
        "    filepath = os.path.join('data', filename)\n",
        "    with open(filepath, 'w', encoding='utf-8') as file:\n",
        "        file.write(cleaned_text)\n",
        "\n",
        "    print(f\"Metin '{filepath}' dosyasına kaydedildi.\")\n"
      ],
      "metadata": {
        "id": "2yXPY84Byp9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data klasörüne bak data.txt diye şeyap\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Data klasöründeki tüm .txt dosyalarını listele\n",
        "data_folder = 'data'\n",
        "text_files = [f for f in os.listdir(data_folder) if f.endswith('.txt')]\n",
        "\n",
        "# Tüm txt dosyalarını yükle ve içeriklerini birleştir\n",
        "all_text_data = \"\"\n",
        "for file_name in text_files:\n",
        "    file_path = os.path.join(data_folder, file_name)\n",
        "    print(f\"Şu an {file_name} ile işlem yapıyorum.\")\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        all_text_data += file.read() + \"\\n\"\n",
        "\n",
        "# Köşeli parantez içindeki tüm rakamları sil\n",
        "cleaned_text = re.sub(r'\\[\\d+\\]', '', all_text_data)\n",
        "\n",
        "# Boş satırları temizle\n",
        "cleaned_text = re.sub(r'\\n\\s*\\n', '\\n', cleaned_text)\n",
        "\n",
        "# Temizlenmiş datayı data.txt olarak kaydet\n",
        "with open(\"data.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(cleaned_text.strip())  # Strip, başındaki ve sonundaki boşlukları da temizler\n",
        "\n",
        "print(\"Tüm veriler temizlendi ve data.txt dosyasına kaydedildi.\")\n"
      ],
      "metadata": {
        "id": "XUZvURogl63t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7z2UzPZEQP9"
      },
      "outputs": [],
      "source": [
        "# Eğitimi burda hallediyoruz\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "\n",
        "# Model ve tokenizer'ı yükle\n",
        "model_name = \"gpt2\"  # veya daha küçük bir versiyonu: \"gpt2-small\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Veri kümesini oluştur\n",
        "def load_dataset(file_path, tokenizer, block_size=128):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=file_path,\n",
        "        block_size=block_size,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "# data.txt dosyasını oku ve veri kümesini yükle\n",
        "train_dataset = load_dataset(\"data.txt\", tokenizer)\n",
        "\n",
        "# Veri hazırlayıcı ve eğitim argümanlarını ayarla\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./ahraz\",  # Modelin kaydedileceği dizin\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,  # Eğitim döngüsü sayısı\n",
        "    per_device_train_batch_size=2,  # Eğitim sırasında her cihazda batch boyutu\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Trainer ile modeli eğit\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Modeli kaydet\n",
        "model.save_pretrained(\"./ahraz\")\n",
        "tokenizer.save_pretrained(\"./ahraz\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#burda konuşmaç\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Model ve tokenizer'ı yükle\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"./ahraz\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./ahraz\")\n",
        "\n",
        "def chat(input_text):\n",
        "    # Girişleri encode et\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "    # Attention mask oluştur\n",
        "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)\n",
        "\n",
        "    # Modeli çalıştır\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        attention_mask=attention_mask,  # Attention mask'ı ekle\n",
        "        max_length=80,      # Yanıtın maksimum uzunluğu\n",
        "        num_return_sequences=1,  # Döndürülecek yanıt sayısı\n",
        "        do_sample=True,      # Sampling (örnekleme) modunu aktif et\n",
        "        temperature=0.7,     # Rastgelelik oranı\n",
        "        top_k=50,            # En yüksek olasılıklı token'ları dikkate al\n",
        "        top_p=0.85,          # Nucleus sampling için olasılık eşikleri\n",
        "        pad_token_id=tokenizer.eos_token_id  # Padding token'ı ayarla\n",
        "    )\n",
        "\n",
        "    # Yanıtı decode et\n",
        "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Soruyu yanıtın başından çıkar\n",
        "    response = decoded_output[len(input_text):].strip()\n",
        "    #response = decoded_output\n",
        "\n",
        "    return response\n",
        "\n",
        "# Sohbeti başlat\n",
        "while True:\n",
        "    user_input = input(\"you: \")\n",
        "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "        print(\"ahraz: Görüşürüz!\")\n",
        "        break\n",
        "    response = chat(user_input)\n",
        "    print(\"ahraz:\", response)\n"
      ],
      "metadata": {
        "id": "Ktx31u2LuySt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}